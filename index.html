<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Felix Yanwei Wang</title>

  <meta name="author" content="Felix Yanwei Wang">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/seal_icon.png">
</head>

<body>
  <table style="width:100%;max-width:1000px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Felix Yanwei Wang</name>
              </p>
              <p><font color="red">I am on the job market for research scientist positions starting in spring 2025. </font></p>
              <p>
                I am <!--a <strong> happy </strong> (<a href="https://www.mycaltucker.com/">not just me</a>)-->a final-year EECS PhD student at <a href="https://www.csail.mit.edu/">MIT CSAIL</a>, working with <a href="https://aeroastro.mit.edu/people/julie-shah/">Julie Shah</a> on robot learning, specifically, inference-time policy alignment through human interactions. I am also a fellow at <a href="https://ipc.mit.edu/people/felix-wang/">MIT's Work of the Future</a> group focusing on generative AI.
              </p>
              <p>
                Before MIT, I did my MS in robotics at Northwestern University and researched with <a href="https://robotics.northwestern.edu/people/profiles/faculty/murphey-todd.html">Todd Murphey</a> and <a href="https://robotics.northwestern.edu/people/profiles/faculty/hartmann-mitra.html">Mitra Hartmann</a> on active sensing. I completed my undergraduate degree in physics at Middlebury College with <a href="https://www.middlebury.edu/institute/people/richard-wolfson">Richard Wolfson</a>. 
              </p>
              <p> Outside research, I enjoy <a href="https://people.csail.mit.edu/felixw/theatre/">theatre</a> and backpacking. I <a href="https://youtu.be/OU6qPbvcKG0">thru-hiked PCT</a> in 2019.
              </p>
              <p> 
                <a href="mailto:felixw@mit.edu">Email</a>
                /
                <a href="https://scholar.google.com/citations?user=M6HrtLUAAAAJ&hl=en">Google Scholar</a>
                /
                <a href="https://www.linkedin.com/in/felix-yanwei-wang">LinkedIn</a>
                /
                <a href="Felix_Wang_CV.pdf">CV</a>
                /
                <a href="https://twitter.com/felixwyw">X</a>
              </p>
              <p style="text-align:center">
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/felix.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/felix.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>News</heading>
                <ul>
                  <li><b>[Dec 2024]</b>
                    Invited talk at MIT <a href="https://ei.csail.mit.edu/seminars.html">EI Seminar</a> <font color="lightgray">- "Inference-Time Policy Customization through Interactive Task Specification"</font>
                  <li><b>[Oct 2024]</b>
                    Invited talk at University of New Hampshire <font color="lightgray">- "Inference-Time Policy Alignment through Human Interactions"</font>
                  </li>
                  <!-- <li><b>[Jul 2024]</b>
                    Invited talk at Fauna Robotics <font color="lightgray">- "Conditional Motion Generation through Physical Interactions"</font>
                  </li> -->
                  <li><b>[Jul 2024]</b>
                    Organized <a href="https://sites.google.com/view/gai-hri/home">GenAI-HRI workshop</a> at RSS 2024.</li>
                  </li>
                  <!-- <li><b>[Mar 2024]</b>
                    <a href="https://news.mit.edu/2024/engineering-household-robots-have-little-common-sense-0325">MIT News coverage</a> of our ICLR 2024 paper on grounding language plans in demonstrations. </li> -->
                  <li><b>[Mar 2024]</b>
                    Invited talk at Brown University <a href="https://h2r.github.io/brown-lab-talks/">Robotics Seminar</a> <font color="lightgray">- "Interactive Task and Motion Imitation"</font> </li>
                  <li><b>[Mar 2024]</b>
                    Invited talk at University of Utah <font color="lightgray">- "Interactive Task and Motion Imitation"</font> </li>
                  <li><b>[Jan 2024]</b>
                    Check out our <a href="https://mitgenerationai.substack.com/p/welcome-to-generation-ai">Generative AI newsletter</a> from MIT's work of the future group. </li>
                  <li><b>[Oct 2023]</b> 
                    Temporal logic imitation was awarded the <font color="red">Best Student Paper</font> at IROS 2023 Workshop. </li>
                  <!-- <li><b>[Sep 2023]</b>
                    Selected as <a href="https://ipc.mit.edu/people/felix-wang/">Work of the Future Fellow</a> in Generative AI.</li> -->
                  <li><b>[Jan 2023]</b>
                    <a href="https://twitter.com/NewsHour/status/1610425346259619840">PBS News coverage</a> of our human robot interaction <a href="https://yanweiw.github.io/tli/#Museum">exhibition</a> at MIT museum. </li>
                </ul>
              <br>

              <heading>Research</heading>
              <p>
              Imagine driving with Google Maps, where multiple routes unfold before you. As you take turns and change plans, it adapts instantly recalculating to match your shifting preferences. <font color="red">My research goal</font> is to bring this level of interactivity to multimodal embodied AI, empowering humans to steer pre-trained foundation models at inference-time.
              </p>
            </td>
          </tr>
        </tbody></table>
      
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

        <tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
            <div class="one">
              <img src='images/itps.png' width="100%">
            </div>
          </td>
          <td style="padding:20px;width:75%;vertical-align:middle">
            <a href="https://yanweiw.github.io/itps/">
              <papertitle>Inference-Time Policy Steering through Human Interactions</papertitle>
            </a>
            <br>
            <strong>Yanwei Wang</strong>,
            Lirui Wang,
            Yilun Du,
            Balakumar Sundaralingam,
            Xuning Yang,
            Yu-Wei Chao,
            Claudia Perez-D'Arpino,
            Dieter Fox,
            Julie Shah
            <em><br>
              <a href="https://arxiv.org/abs/2411.16627">arxiv</a> /
              <a href="https://github.com/yanweiw/itps">code</a> /
              <a href="https://yanweiw.github.io/itps/">project page</a> / 
              <a href="https://x.com/felixwyw/status/1864021820304937028">twitter</a>
              <br>
              <strong>ICRA 2025</strong>
            </em><br>
            <p>We propose Inference-Time Policy Steering (ITPS), a framework that leverages human interactions to zero-shot adapt
            pre-trained generative policies for downstream tasks without any additional data collection or fine-tuning.</p>
          </td>

        <tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
            <div class="one">
              <img src='images/vmi.png' width="100%">
            </div>
          </td>
          <td style="padding:20px;width:75%;vertical-align:middle">
            <a href="https://arxiv.org/abs/2410.19141v1">
              <papertitle>Versatile Demonstration Interface: Toward More
              Flexible Robot Demonstration Collection</papertitle>
            </a>
            <br>
            Michael Hagenow,
            Dimosthenis Kontogiorgos,
            <strong>Yanwei Wang</strong>,
            Julie Shah
            <em><br>
              <a href="https://arxiv.org/abs/2410.19141v1">arxiv</a>
              <!-- <a href="https://github.com/yanweiw/glide">code</a> / -->
              <!-- <a href="https://yanweiw.github.io/glide/">project page</a> -->
              <br>
              In submission
            </em><br>
            <p>We present the Versatile Demonstration Interface (VDI), a collaborative robot tool designed to enable seamless
            transitions between data collection modes—teleoperation, kinesthetic teaching, and natural demonstrations—without the
            need for additional environmental instrumentation.</p>
          </td>

        <tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
            <div class="one">
              <img src='images/scoop.jpg' width="100%">
            </div>
          </td>
          <td style="padding:20px;width:75%;vertical-align:middle">
            <a href="https://yanweiw.github.io/glide/">
              <papertitle>Grounding Language Plans in Demonstrations through Counter-factual Perturbations</papertitle>
            </a>
            <br>
            <strong>Yanwei Wang</strong>,
            Tsun-Hsuan Wang, 
            Jiayuan Mao,
            Michael Hagenow,
            Julie Shah
            <em><br>
              <a href="https://arxiv.org/abs/2403.17124">arxiv</a> /
              <a href="https://github.com/yanweiw/glide">code</a> /
              <a href="https://yanweiw.github.io/glide/">project page</a> /
              <a href="https://news.mit.edu/2024/engineering-household-robots-have-little-common-sense-0325">MIT News</a>
              <br>
              <strong>ICLR 2024</strong> (<strong style="color:red;">Spotlight</strong>, acceptance rate: 5%)
            </em><br>
            <p>This work learns grounding classifiers for LLM planning. Our end-to-end explanation-based network is
            trained to differentiate successful demonstrations from failing counterfactuals and as a by-product learns classifiers that ground continuous states
            into discrete manipulation mode families without dense labeling.</p>
          </td>



          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/robot_1.jpg' width="100%">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://yanweiw.github.io/tli/">
              <papertitle>Temporal Logic Imitation: Learning Plan-Satisficing Motion Policies from Demonstrations</papertitle>
              </a>
              <br>
              <strong>Yanwei Wang</strong>,
              Nadia Figueroa, Shen Li, Ankit Shah, Julie Shah
              <em><br>
              <a href="https://arxiv.org/abs/2206.04632">arxiv</a>
               / 
              <a href="https://github.com/yanweiw/tli">code</a>
              /
              <a href="https://yanweiw.github.io/tli/">project page</a>
              /
              <a href="https://twitter.com/NewsHour/status/1610425346259619840">PBS News</a>
              <br>
              <strong>CoRL 2022</strong> (<strong style="color:red;">Oral</strong>, acceptance rate: 6.5%) <br>
              <strong>IROS 2023 Workshop</strong> (<strong style="color:red;"> Best Student Paper</strong>, Learning Meets Model-based Methods for Manipulation and Grasping Workshop) 
              </em><br>

              <!-- / -->
              <!-- <a href="https://arxiv.org/abs/2106.01970">arXiv</a> -->
              <!-- / -->
              <!-- <a href="https://www.youtube.com/watch?v=UUVSPJlwhPg">video</a> -->
              <!-- <p></p> -->
              <p>We present a continuous motion imitation method that can provably satisfy any discrete plan specified by a Linear Temporal Logic (LTL) formula. Consequently, the imitator is robust to both task- and motion-level disturbances and guaranteed to achieve task success.</p>
            </td>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/drllama.png' width="100%">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/2305.07804.pdf">
                <papertitle>Improving Small Language Models on PubMedQA via Generative Data Augmentation</papertitle>
              </a>
              <br>
              Zhen Guo,
              <strong>Yanwei Wang</strong>,
              Peiqi Wang,
              Shangdi Yu
              <!-- <br> -->
              <!-- <em>arXiv</em>, 2021  -->
              <em><br>
                <a href="https://arxiv.org/pdf/2305.07804.pdf">arxiv</a>
                <br>
                <strong>KDD 2023</strong> (Foundations and Applications in Large-scale
                AI Models Pre-training, Fine-tuning, and Prompt-based Learning Workshop)
              </em><br>
              <p>We prompt large language models to augment a domain-specific dataset to train specialized small language models that outperform the general-purpose LLM.</p>
            </td>


          <tr onmouseout="noise2ptz_stop()" onmouseover="noise2ptz_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <!-- <div class="two" id='noise2ptz_image'>
                  <img src='images/noise2ptz.png' width="100%"></div> -->
                <img src='images/noise2ptz.png' width="100%">
              </div>
              <script type="text/javascript">
                function noise2ptz_start() {
                  document.getElementById('noise2ptz_image').style.opacity = "1";
                }

                function nerfactor_stop() {
                  document.getElementById('noise2ptz_image').style.opacity = "0";
                }
                noise2ptz_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://yanweiw.github.io/noise2ptz/">
              <papertitle>Visual Pre-training for Navigation: What Can We Learn from Noise?</papertitle>
              </a>
              <br>
							<strong>Yanwei Wang</strong>,
              Ching-Yun Ko,
              Pulkit Agrawal
              <!-- <br> -->
              <!-- <em>arXiv</em>, 2021  -->
              <em><br>
              <a href="https://arxiv.org/abs/2207.00052">arxiv</a>
              /
              <a href="https://github.com/yanweiw/noise2ptz">code</a>
              /
              <a href="https://yanweiw.github.io/noise2ptz">project page</a><br>
              <strong>IROS 2023</strong> 
              <br>
              <strong>NeurIPS 2022 Workshop</strong> (Synthetic Data for Empowering ML Research / Self-Supervised Learning)
              </em><br>
              <p>By learning how to pan, tilt and zoom its camera to focus on random crops of a noise image, an embodied agent can pick up navigation skills in realistically simulated environments.</p>
            </td>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <!-- <div class="two" id='info_search_image'> -->
                  <!-- <img src='images/mit_museum.png' width="160" height="160"></div> -->
                <img src='images/mit_museum.png' width="100%">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://yanweiw.github.io/tli/#Museum">
              <papertitle>MIT Museum Interactive Robot Exhibition: Teach a Robot Motions</papertitle>
              </a>
              <br>
              Nadia Figueroa, 
              <strong>Yanwei Wang</strong>,
              Julie Shah
              <p></p>
              <p>We installed an interactive exhibition at <a href="https://mitmuseum.mit.edu/">MIT Museum</a> that allows non-robot-experts to teach a robot an inspection task using demonstrations. The robustness and compliance of the learned motion policy enables visitors (including kids) to physically perturb the system safely 24/7 without losing a success gaurantee.</p>
            </td>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <!-- <div class="two" id='info_search_image'>
                  <img src='images/info.png' width="160" height="160"></div> -->
                <img src='images/info.png' width="100%">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://github.com/yanweiw/infotaxis">
              <papertitle>Active Learning for Object Search</papertitle>
              </a>
              <br>
              <strong>Yanwei Wang</strong>,
              Todd Murphey
              <p></p>
              <p>This research project studies the application of two information gain methods in a search problem: 1. Infotaxis - Search guided by entropy minimization 2. Ergodic exploration - Search guided by proportional coverage.</p>
            </td>



          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <!-- <div class="two" id='active_whisking_image'>
                  <img src='images/rat.png' width="160"></div> -->
                <img src='images/rat.png' width="100%">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://github.com/yanweiw/dqnActiveWhisking">
              <papertitle>Active Sensing with Tactile Sensors</papertitle>
              </a>
              <br>
              <strong>Yanwei Wang</strong>,
              Mitra Hartmann
              <p></p>
              <p>This research project studies how active sensing, i.e., choosing what data to collect, can improve data efficiency for decision-making under uncertainty. Inspired by the active whisking behavior of rats, we use simulated rat whisker sensory signals as a model for spatial-temporal data to learn policies that first collect observations and then classify object shapes.</p>
            </td>


          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/starfish.png' width="100%" height="100%">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://github.com/yanweiw/picco">
              <papertitle>Multi-agent Distributed Sensing and Control</papertitle>
              </a>
              <br>
              <strong>Yanwei Wang</strong>,
              Michael Rubinstein
              <p></p>
              <p>This research project studies multi-agent distributed algorithms concerning coordination, segregation, and locomotion, with a hardware implementation of robust localization with cheap sensors on a low-cost underactuated system.</p>
            </td>


          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/dwg-crop.jpg' width="100%" height="100%">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://github.com/yanweiw/dwg">
              <papertitle>Analyzing Energy Efficiency of Bio-Inspired Wind Generator</papertitle>
              </a>
              <br>
              <strong>Yanwei Wang</strong>,
              Richard Wolfson
              <br>
              <br>
              <a href="http://people.csail.mit.edu/felixw/dwg/felix_thesis.pdf">pdf</a>
              <p></p>
              <p>For my undergraduate physics thesis at Middlebury College, I did a computational fluid dynamics (CFD) simulation of Festo's Dual-Wing generator using COMSOL CFD package.</p>
            </td>


        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Projects</heading>
              <p>
                Miscellaneous class projects.
              </p>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/3cart.png' width="100%">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://github.com/hjsuh94/soft_contact">
              <papertitle>Planning through Soft Contact</papertitle>
              </a>
              <br>
              Terry Suh,
              <strong>Yanwei Wang</strong>
              <br>
              <br>
              <a href="http://people.csail.mit.edu/felixw/soft_contact/soft_contact.pdf">pdf</a>
              <p></p>
              <p>We compare the effectiveness of relaxation methods for warm starting trajectory optimization through soft contact.</p>
            </td>

<!--           <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/sawyer.png' width="160" height="160">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://github.com/yanweiw/sawyer_beer">
              <papertitle>Robot Manipulation with ROS</papertitle>
              </a>
            </td>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/quad.png' width="160" height="160">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://github.com/yanweiw/quad_SE3">
              <papertitle>Simulating Quadrotor Dynamics on SE(3) Manifold</papertitle>
              </a>
            </td> -->

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/lane.png' width="100%" height="100%">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://github.com/willshw/lane-detection">
              <papertitle>Lane Detection for Self-Driving Applications</papertitle>
              </a>
            </td>

        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
                <a href="https://jonbarron.info/">Template</a>
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>
